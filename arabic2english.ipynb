{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92885db",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e746c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#to open csv file\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sentences & words tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#regular expression \n",
    "import re\n",
    "#for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb6d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hends\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f712df6",
   "metadata": {},
   "source": [
    "# csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc8806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arabic</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>متى أنشئت هذه الجامعة؟</td>\n",
       "      <td>When was this university founded?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أراها نادراً</td>\n",
       "      <td>I see it rarely.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يعزف على البيانو بشكل جيد جداً</td>\n",
       "      <td>He plays the piano very well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مع كل احترامي.</td>\n",
       "      <td>With all due respect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نظف أسنانك</td>\n",
       "      <td>Brush your teeth clean.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           arabic                            english\n",
       "0          متى أنشئت هذه الجامعة؟  When was this university founded?\n",
       "1                    أراها نادراً                   I see it rarely.\n",
       "2  يعزف على البيانو بشكل جيد جداً      He plays the piano very well.\n",
       "3                  مع كل احترامي.              With all due respect.\n",
       "4                      نظف أسنانك            Brush your teeth clean."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = r'C:\\Users\\hends\\Documents\\ANLP\\dataset\\arabic_english.csv'\n",
    "dataset = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a7056",
   "metadata": {},
   "source": [
    "# preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6fe42",
   "metadata": {},
   "source": [
    "### arabic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f109601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove Arabic stopwords\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove Arabic punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Apply stemming\n",
    "    stemmer = ISRIStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_preprocessed'] = dataset['arabic'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c1cd216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [شئت, جمع]\n",
       "1                       [ارا, ندر]\n",
       "2        [عزف, ينو, شكل, جيد, جدا]\n",
       "3                            [حرم]\n",
       "4                       [نظف, سنن]\n",
       "                   ...            \n",
       "34884    [ذهب, انت, يسر, ونأ, يمي]\n",
       "34885         [يجب, قرء, لنص, بتأ]\n",
       "34886                   [عند, شكل]\n",
       "34887                   [عند, شكل]\n",
       "34888                   [عند, شكل]\n",
       "Name: arabic_preprocessed, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e09b2",
   "metadata": {},
   "source": [
    "### english preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove English punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5176fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_preprocessed'] = dataset['english'].apply(preprocess_english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "579ef3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [university, founded]\n",
       "1                          [see, rarely]\n",
       "2                    [play, piano, well]\n",
       "3                         [due, respect]\n",
       "4                  [brush, teeth, clean]\n",
       "                      ...               \n",
       "34884              [go, left, go, right]\n",
       "34885    [must, read, textbook, closely]\n",
       "34886                          [problem]\n",
       "34887                 [ve, got, problem]\n",
       "34888                 [ve, got, problem]\n",
       "Name: english_preprocessed, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76736404",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd6b2f",
   "metadata": {},
   "source": [
    "### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec2487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming you have lists of tokenized Arabic and English sentences\n",
    "arabic_corpus = [sentence for sentence in dataset['arabic_preprocessed']]\n",
    "english_corpus = [sentence for sentence in dataset['english_preprocessed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f46e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Arabic Word2Vec model\n",
    "arabic_model = Word2Vec(sentences=arabic_corpus, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Train English Word2Vec model\n",
    "english_model = Word2Vec(sentences=english_corpus, vector_size=300, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model.save('arabic_word2vec.model')\n",
    "english_model.save('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89becdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "arabic_model = Word2Vec.load('arabic_word2vec.model')\n",
    "english_model = Word2Vec.load('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf8d030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_embeddings(text, model):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f49b06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_embeddings'] = dataset['arabic_preprocessed'].apply(lambda x: text_to_embeddings(x, arabic_model))\n",
    "dataset['english_embeddings'] = dataset['english_preprocessed'].apply(lambda x: text_to_embeddings(x, english_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16cc25da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.011475698, 0.10967453, 0.02558354, 0.02932...\n",
       "1        [[0.0051676948, 0.061253097, 0.017778201, 0.01...\n",
       "2        [[0.031647027, 0.21455647, 0.054307714, 0.0547...\n",
       "3        [[0.028915202, 0.23491731, 0.056054126, 0.0598...\n",
       "4        [[0.02441435, 0.23771553, 0.051473312, 0.06183...\n",
       "                               ...                        \n",
       "34884    [[0.02755453996360302, 0.2768106162548065, 0.0...\n",
       "34885    [[0.03508031368255615, 0.2866920530796051, 0.0...\n",
       "34886    [[0.037276633, 0.30006623, 0.06619843, 0.07606...\n",
       "34887    [[0.037276633, 0.30006623, 0.06619843, 0.07606...\n",
       "34888    [[0.037276633, 0.30006623, 0.06619843, 0.07606...\n",
       "Name: arabic_embeddings, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b82e4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.022330971, 0.21512741, -0.010423325, 0.108...\n",
       "1        [[0.015965493, 0.23092939, -0.0027182873, 0.11...\n",
       "2        [[0.026843045, 0.23262744, -0.016636169, 0.112...\n",
       "3        [[0.009102348, 0.08759432, -0.0013995869, 0.04...\n",
       "4        [[0.009990926, 0.09358135, -0.0077565066, 0.04...\n",
       "                               ...                        \n",
       "34884    [[0.01925363, 0.22731914, -0.014380923, 0.1263...\n",
       "34885    [[0.030479565, 0.26141086, -0.00827911, 0.1375...\n",
       "34886    [[0.020079298, 0.24023326, -0.007571408, 0.120...\n",
       "34887    [[0.028377939, 0.24865027, -0.020709172, 0.137...\n",
       "34888    [[0.028377939, 0.24865027, -0.020709172, 0.137...\n",
       "Name: english_embeddings, Length: 34889, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cbc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
